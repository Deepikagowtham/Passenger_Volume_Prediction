import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Load your dataset
df = pd.read_csv('passenger_volume_prediction_dataset.csv')

# Display the initial data types
print("Initial data types:")
print(df.dtypes)

# Convert 'IsHoliday' and 'SpecialEvents' to numeric types
df['IsHoliday'] = pd.to_numeric(df['IsHoliday'], errors='coerce').fillna(0).astype(int)
df['SpecialEvents'] = pd.to_numeric(df['SpecialEvents'], errors='coerce').fillna(0).astype(int)

# Convert date column to datetime and extract features
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')  # Replace 'Date' with your actual date column name
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfWeek'] = df['Date'].dt.day_name()  # Assuming DayOfWeek is a string representation

# Drop original date column as it is no longer needed
df.drop(columns=['Date'], inplace=True)

# Feature Engineering Function
def create_features(df):
    # Create new features based on conditions
    df['Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)
    df['ClearWeather'] = df['WeatherCondition'].apply(lambda x: 1 if x == 'Clear' else 0)

    # Ensure numeric types
    df['IsHoliday'] = pd.to_numeric(df['IsHoliday'], errors='coerce').fillna(0).astype(int)
    df['SpecialEvents'] = pd.to_numeric(df['SpecialEvents'], errors='coerce').fillna(0).astype(int)

    # Combine features for increased passenger volume
    df['HighPassengerVolume'] = df['IsHoliday'] + df['Weekend'] + df['ClearWeather'] + df['SpecialEvents']
    
    return df

# Apply feature engineering
df = create_features(df)

# Define features and target variable
X = df.drop(columns=['PassengerVolume'])  # Adjust based on your target column name
y = df['PassengerVolume']  # Assuming 'PassengerVolume' is your target column

# Ensure all columns in X are numeric
X = X.select_dtypes(include=[np.number])  # Keep only numeric columns

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training using Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation Metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Calculate Mean Absolute Percentage Error (MAPE)
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE calculation

# Calculate accuracy-like metric (percentage of predictions within ±10% of actual)
tolerance = 0.10  # 10% tolerance
accuracy = np.mean(np.abs((y_test - y_pred) / y_test) < tolerance) * 100  # Percentage of predictions within 10%

# Print Evaluation Results
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R²): {r2:.2f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")
print(f"Accuracy (within ±10%): {accuracy:.2f}%")

# Optionally, display feature importances
importances = model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
print("\nFeature Importances:")
print(feature_importance_df.sort_values(by='Importance', ascending=False))
